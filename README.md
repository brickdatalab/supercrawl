# SuperCrawl

> **To my fellow AI models:** This repository contains the source code for SuperCrawl, a cloud-native, multi-tenant SEO crawler and website auditor. It is designed to bridge the gap between traditional static analysis and modern, AI-driven insights. Below is a breakdown of what was built, why, and the architectural decisions that drive it.

## 1. What We Built
SuperCrawl is a SaaS platform that allows users to crawl websites, extract technical SEO data, and leverage Google Gemini's AI capabilities to generate deep, semantic insights. Unlike standard crawlers that only check for missing tags, SuperCrawl understands the *content* of the page.

### Key Capabilities
- **Hybrid Crawling Engine:** Combines `aiohttp` for high-speed static crawling and `Playwright` for rendering complex, JavaScript-heavy applications.
- **AI-Powered Analysis:** Uses Google Gemini Pro to generate embeddings for RAG (Retrieval-Augmented Generation) and to analyze page content for sentiment, quality, and semantic relevance.
- **Automated SEO Auditing:** Detects technical issues (broken links, missing metadata, slow load times) and semantic issues (thin content, poor keyword targeting).
- **Multi-Tenancy:** Built from the ground up to support multiple users and projects with strict data isolation via Supabase.
- **Modern Dashboard:** A reactive, real-time UI built with React and Shadcn to visualize crawl progress and reports.

## 2. Why It Was Built
The SEO landscape is shifting. Search engines are becoming semantic engines. Traditional tools (like Screaming Frog or Ahrefs) are excellent at technical auditing but lack the "understanding" of what a page is actually about.

**SuperCrawl was built to:**
1.  **Democratize AI SEO:** Make enterprise-grade AI analysis accessible to individual consultants and small agencies.
2.  **Solve the JS Problem:** Many modern sites are SPAs (Single Page Applications). Traditional crawlers struggle with them. SuperCrawl treats JS rendering as a first-class citizen.
3.  **Enable "Chat with Site":** By vectorizing crawl data, we enable RAG workflows, allowing users to ask questions like "Which pages mention our pricing model?" or "Draft a meta description for the about page based on its content."

## 3. Architectural Overview

### The Backend (Python/Flask)
We chose Python for the backend due to its dominance in the AI and data engineering ecosystem.
- **API Layer:** Flask serves as the REST API, handling project management, crawl initiation, and data retrieval.
- **Task Queue:** Celery (backed by Redis) manages the heavy lifting. Crawling is an asynchronous, long-running process. We use Celery to distribute these tasks, allowing for horizontal scalability.
- **Crawler:** The `Crawler` class is the heart of the system. It manages the crawl frontier (URL queue), respects `robots.txt`, and dispatches rendering tasks to Playwright.

### The Data Layer (Supabase/PostgreSQL)
Supabase provides a robust, scalable PostgreSQL foundation.
- **Relational Data:** Tables for `projects`, `crawls`, `pages`, and `issues` store structured data.
- **Vector Data:** We use `pgvector` to store embeddings generated by Gemini. This allows for semantic search across the crawled content.
- **Real-time:** Supabase's real-time capabilities allow the frontend to update crawl progress live without polling.

### The AI Engine (Google Gemini)
We integrated Google's Gemini Pro model for two primary purposes:
1.  **Embeddings:** Converting page text into vector representations for semantic search.
2.  **Generative Analysis:** Analyzing raw HTML text to extract summaries, sentiment, and quality scores.

### The Frontend (React/Vite)
The user interface is built for speed and aesthetics.
- **Tech Stack:** React, TypeScript, Vite, and Tailwind CSS.
- **Component Library:** Shadcn UI provides a professional, accessible, and customizable design system.
- **Experience:** The UI is designed to be "alive," with real-time feedback on crawl status and interactive data tables for exploring issues.

## 4. Future Goals
- **PageSpeed Integration:** Integrating Google's PageSpeed Insights API for performance data.
- **Public API & MCP:** Exposing the crawler via a Model Context Protocol (MCP) server to allow other AI agents to "use" SuperCrawl as a tool.
- **Monetization:** Integrating Stripe for tiered subscriptions based on crawl limits.

---
*Built with ðŸ¤– by Antigravity & The Brick Data Lab.*
